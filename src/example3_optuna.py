import os
import threading
import time
import webbrowser
from typing import Dict, List, Optional

import optuna
from tqdm import tqdm

try:
    import optuna_dashboard
except ImportError:  # pragma: no cover
    optuna_dashboard = None  # type: ignore

from manuscript.recognizers import TRBA
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    compute_accuracy,
)

# === User-configurable paths & parameters ===
IMAGE_DIR = r"C:\shared\Archive_19_04\data_archive\test"
GT_FILE = r"C:\shared\Archive_19_04\data_archive\gt_test - Copy.txt"
MODEL_PATH = r"C:\shared\exp1_model_64\best_acc_ckpt.pth"
CONFIG_PATH = r"C:\shared\exp1_model_64\config.json"
CHARSET_PATH: Optional[str] = (
    None  # set to override default charset, otherwise keep None
)
BATCH_SIZE = 128
TRIALS = 100
STUDY_NAME = "trba-decode-search"
DEFAULT_STORAGE_FILE = os.path.join(os.path.dirname(__file__), "optuna_trba.db")
STORAGE: Optional[str] = f"sqlite:///{DEFAULT_STORAGE_FILE}"
SEED = 42

# Dashboard settings (requires optuna-dashboard and persistent storage)
ENABLE_DASHBOARD = True
DASHBOARD_HOST = "127.0.0.1"
DASHBOARD_PORT = 8080


def load_ground_truth(gt_path: str) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    with open(gt_path, "r", encoding="utf-8") as f:
        for line in f:
            parts = line.strip().split("\t", 1)
            if len(parts) == 2:
                fname, text = parts
                mapping[fname.strip()] = text.strip()
    if not mapping:
        raise RuntimeError(f"No ground-truth entries found in {gt_path}")
    return mapping


def make_image_list_from_gt(gt_map: Dict[str, str], image_dir: str) -> List[str]:
    """
    –°—Ç—Ä–æ–∏—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—Ç—Ä–æ–≥–æ –ø–æ –ø–æ—Ä—è–¥–∫—É –∫–ª—é—á–µ–π –≤ gt_map,
    –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏. –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ—Ç –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–∞—Ö.
    """
    images: List[str] = []
    missing: List[str] = []

    for fname in gt_map.keys():  # –ø–æ—Ä—è–¥–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç—Ä–æ–∫–∞–º –≤ GT_FILE (Py3.7+)
        path = os.path.join(image_dir, fname)
        if os.path.isfile(path):
            images.append(path)
        else:
            missing.append(fname)

    if missing:
        print(
            f"[Warn] –í GT —É–∫–∞–∑–∞–Ω–æ {len(missing)} —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø–∞–ø–∫–µ '{image_dir}':"
        )
        for m in missing[:20]:
            print("   -", m)
        if len(missing) > 20:
            print(f"   ... –∏ –µ—â—ë {len(missing) - 20}")

    if not images:
        raise RuntimeError("–ü–æ GT –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

    return images


def collect_images(image_dir: str) -> List[str]:
    valid_ext = {".png", ".jpg", ".jpeg", ".tif", ".tiff", ".bmp"}
    images: List[str] = []
    for fname in os.listdir(image_dir):
        path = os.path.join(image_dir, fname)
        if not os.path.isfile(path):
            continue
        if os.path.splitext(fname)[1].lower() in valid_ext:
            images.append(path)
    images.sort()
    if not images:
        raise RuntimeError(f"No images with valid extensions found in {image_dir}")
    return images


def evaluate_avg_cer(
    recognizer: TRBA,
    images: List[str],
    gt_map: Dict[str, str],
    batch_size: int,
    *,
    mode: str,
    beam_size: int,
    alpha: float,
    temperature: float = 1.0,
) -> float:
    num_batches = (len(images) + batch_size - 1) // batch_size

    print(
        f"  üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {num_batches} –±–∞—Ç—á–µ–π "
        f"(batch_size={batch_size}, mode={mode}, beam_size={beam_size})"
    )

    # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –±–∞—Ç—á–µ–π
    with tqdm(
        total=num_batches, desc="  –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π", unit="batch", leave=False
    ) as pbar:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        all_predictions = []
        for i in range(0, len(images), batch_size):
            batch_images = images[i : i + batch_size]
            batch_predictions = recognizer.predict(
                images=batch_images,
                batch_size=batch_size,
                mode=mode,
                beam_size=beam_size,
                alpha=alpha,
                temperature=temperature,
            )
            all_predictions.extend(batch_predictions)
            pbar.update(1)

    predictions = all_predictions
    print(f"  ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã, –≤—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏...")

    total_cer = 0.0
    count = 0
    for path, (pred_text, _) in zip(images, predictions):
        ref = gt_map.get(os.path.basename(path))
        if ref is None:
            continue
        total_cer += character_error_rate(ref, pred_text)
        count += 1

    if count == 0:
        raise RuntimeError("No predictions matched the ground-truth keys.")
    return total_cer / count


def evaluate_metrics(
    recognizer: TRBA,
    images: List[str],
    gt_map: Dict[str, str],
    batch_size: int,
    *,
    mode: str,
    beam_size: int,
    alpha: float,
    temperature: float,
) -> tuple[float, float]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (avg_cer, accuracy).
    accuracy - –¥–æ–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –≤—ã—á–∏—Å–ª–µ–Ω–Ω–∞—è —á–µ—Ä–µ–∑ compute_accuracy.
    """
    num_batches = (len(images) + batch_size - 1) // batch_size

    print(
        f"  üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {num_batches} –±–∞—Ç—á–µ–π "
        f"(batch_size={batch_size}, mode={mode}, beam_size={beam_size})"
    )

    # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –±–∞—Ç—á–µ–π
    with tqdm(
        total=num_batches, desc="  –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π", unit="batch", leave=False
    ) as pbar:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        all_predictions = []
        for i in range(0, len(images), batch_size):
            batch_images = images[i : i + batch_size]
            batch_predictions = recognizer.predict(
                images=batch_images,
                batch_size=batch_size,
                mode=mode,
                beam_size=beam_size,
                alpha=alpha,
                temperature=temperature,
            )
            all_predictions.extend(batch_predictions)
            pbar.update(1)

    predictions = all_predictions
    print(f"  ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã, –≤—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏...")

    refs: List[str] = []
    hyps: List[str] = []
    total_cer = 0.0

    for path, (pred_text, _) in zip(images, predictions):
        ref = gt_map.get(os.path.basename(path))
        if ref is None:
            continue
        refs.append(ref)
        hyps.append(pred_text)
        total_cer += character_error_rate(ref, pred_text)

    if len(refs) == 0:
        raise RuntimeError("No predictions matched the ground-truth keys.")

    avg_cer = total_cer / len(refs)
    accuracy = compute_accuracy(refs, hyps)
    return avg_cer, accuracy


def maybe_launch_dashboard(study: optuna.Study) -> None:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç optuna-dashboard –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
    """
    global optuna_dashboard  # ‚úÖ —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é

    if optuna_dashboard is None:
        try:
            import optuna_dashboard  # type: ignore
        except ImportError:
            print(
                "[Dashboard] –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω optuna-dashboard. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏: pip install optuna-dashboard"
            )
            return

    if STORAGE is None:
        print(
            "[Dashboard] STORAGE –Ω–µ –∑–∞–¥–∞–Ω. "
            "–ó–∞–¥–∞–π –ø—É—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä: STORAGE = 'sqlite:///trba_decode.db'"
        )
        return

    def _run_server():
        try:
            # optuna-dashboard —Ç—Ä–µ–±—É–µ—Ç storage URL, –∞ –Ω–µ –æ–±—ä–µ–∫—Ç study
            optuna_dashboard.run_server(
                STORAGE,
                host=DASHBOARD_HOST,
                port=DASHBOARD_PORT,
            )
        except Exception as e:
            print(f"[Dashboard] –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {e}")
            import traceback

            traceback.print_exc()

    # üî• –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ä—Ç Dashboard –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    server_thread = threading.Thread(target=_run_server, daemon=True)
    server_thread.start()
    print(f"üöÄ Dashboard –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ http://{DASHBOARD_HOST}:{DASHBOARD_PORT}")
    print(f"üì¶ Storage: {STORAGE}")

    # –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã —Å–µ—Ä–≤–µ—Ä —É—Å–ø–µ–ª –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è
    print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞...")
    time.sleep(3)

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –¥–∞—à–±–æ—Ä–¥–∞ –≤ –±—Ä–∞—É–∑–µ—Ä–µ
    dashboard_url = f"http://{DASHBOARD_HOST}:{DASHBOARD_PORT}"
    try:
        webbrowser.open(dashboard_url)
        print(f"üåê –ë—Ä–∞—É–∑–µ—Ä –æ—Ç–∫—Ä—ã—Ç –Ω–∞ {dashboard_url}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä: {e}")
        print(f"   –û—Ç–∫—Ä–æ–π—Ç–µ –≤—Ä—É—á–Ω—É—é: {dashboard_url}")


def main():
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ ground truth...")
    gt_map = load_ground_truth(GT_FILE)
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(gt_map)} –∑–∞–ø–∏—Å–µ–π ground truth")

    print("üñºÔ∏è  –°–±–æ—Ä–∫–∞ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    images = make_image_list_from_gt(gt_map, IMAGE_DIR)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n")

    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TRBA...")
    recognizer = TRBA(
        model_path=MODEL_PATH,
        config_path=CONFIG_PATH,
        charset_path=CHARSET_PATH,
    )
    print("   –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

    baseline_params = {
        "mode": "greedy",
        "beam_size": 1,
        "alpha": 0.0,
    }
    print(f"\n[Baseline evaluation] –ó–∞–ø—É—Å–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {baseline_params}")
    baseline_cer = evaluate_avg_cer(
        recognizer,
        images,
        gt_map,
        BATCH_SIZE,
        **baseline_params,
    )
    print(f"[Baseline] mode=greedy Avg CER={baseline_cer:.4f}\n")

    sampler = optuna.samplers.TPESampler(seed=SEED)
    study_kwargs = {
        "study_name": STUDY_NAME,
        "direction": "maximize",
        "sampler": sampler,
    }
    if STORAGE:
        study_kwargs["storage"] = STORAGE
        study_kwargs["load_if_exists"] = True

    study = optuna.create_study(**study_kwargs)
    study.enqueue_trial(baseline_params)
    maybe_launch_dashboard(study)

    def objective(trial: optuna.Trial) -> float:
        mode = trial.suggest_categorical("mode", ["greedy", "beam"])

        if mode == "greedy":
            # –î–ª—è greedy —Ä–µ–∂–∏–º–∞ beam-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã
            beam_size = 1
            alpha = 0.0
            temperature = 1.0
            # normalize_by_length = True
            # diverse_groups = 1
            # diversity_strength = 0.0
            # noise_level = 0.3
            # topk_sampling_steps = 3
            # topk = 5
            # coverage_penalty_weight = 0.1
            # expand_beam_steps = 3
        else:
            # –î–ª—è beam —Ä–µ–∂–∏–º–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            beam_size = trial.suggest_int("beam_size", 2, 12)
            alpha = trial.suggest_float("alpha", 0.0, 1.0)
            temperature = trial.suggest_float("temperature", 0.7, 2.0)
            # normalize_by_length = True
            # diverse_groups = trial.suggest_int("diverse_groups", 1, 4)
            # diversity_strength = trial.suggest_float("diversity_strength", 0.0, 2.0)
            # noise_level = trial.suggest_float("noise_level", 0.0, 1.0)
            # topk_sampling_steps = trial.suggest_int("topk_sampling_steps", 0, 10)
            # topk = trial.suggest_int("topk", 1, 20)
            # coverage_penalty_weight = trial.suggest_float(
            #     "coverage_penalty_weight", 0.0, 1.0
            # )
            # expand_beam_steps = trial.suggest_int("expand_beam_steps", 0, 10)

        avg_cer, accuracy = evaluate_metrics(
            recognizer,
            images,
            gt_map,
            BATCH_SIZE,
            mode=mode,
            beam_size=beam_size,
            alpha=alpha,
            temperature=temperature,
        )

        if mode == "greedy":
            print(
                f"[Trial {trial.number}] mode=greedy -> CER={avg_cer:.4f} Acc={accuracy:.4f}"
            )
        else:
            print(
                f"[Trial {trial.number}] mode=beam beam={beam_size} alpha={alpha:.2f} "
                f"-> CER={avg_cer:.4f} Acc={accuracy:.4f}"
            )
        return accuracy

    study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)

    best_params = study.best_params
    best_value = study.best_value
    print("\n=== Optuna Results ===")
    print(f"Best accuracy: {best_value:.4f}")
    for key, value in best_params.items():
        print(f"{key}: {value}")

    print("\nRe-evaluating best parameters...")
    best_cer, best_accuracy = evaluate_metrics(
        recognizer,
        images,
        gt_map,
        BATCH_SIZE,
        mode=best_params.get("mode", "beam"),
        beam_size=best_params.get("beam_size", 5),
        alpha=best_params.get("alpha", 0.0),
        temperature=best_params.get("temperature", 1.0),
    )
    print(f"Confirmed CER: {best_cer:.4f}    Confirmed accuracy: {best_accuracy:.4f}")

    if ENABLE_DASHBOARD and optuna_dashboard is not None and STORAGE is not None:
        print(
            f"[Dashboard] Keep monitoring at http://{DASHBOARD_HOST}:{DASHBOARD_PORT} "
            "or stop the script to close it."
        )


if __name__ == "__main__":
    main()
