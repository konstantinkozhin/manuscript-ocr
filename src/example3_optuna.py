import os
import threading
import time
import webbrowser
from typing import Dict, List, Optional

import optuna
from tqdm import tqdm

try:
    import optuna_dashboard
except ImportError:  # pragma: no cover
    optuna_dashboard = None  # type: ignore

from manuscript.recognizers import TRBA
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    compute_accuracy,
)

# === User-configurable paths & parameters ===
IMAGE_DIR = r"C:\shared\Archive_19_04\data_archive\test"
GT_FILE = r"C:\shared\Archive_19_04\data_archive\gt_test - Copy.txt"
MODEL_PATH = r"C:\shared\exp1_model_64\best_acc_ckpt.pth"
CONFIG_PATH = r"C:\shared\exp1_model_64\config.json"
CHARSET_PATH: Optional[str] = (
    None  # set to override default charset, otherwise keep None
)
BATCH_SIZE = 128
TRIALS = 100
STUDY_NAME = "trba-decode-search"
DEFAULT_STORAGE_FILE = os.path.join(os.path.dirname(__file__), "optuna_trba.db")
STORAGE: Optional[str] = f"sqlite:///{DEFAULT_STORAGE_FILE}"
SEED = 42

# Dashboard settings (requires optuna-dashboard and persistent storage)
ENABLE_DASHBOARD = True
DASHBOARD_HOST = "127.0.0.1"
DASHBOARD_PORT = 8080


def load_ground_truth(gt_path: str) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    with open(gt_path, "r", encoding="utf-8") as f:
        for line in f:
            parts = line.strip().split("\t", 1)
            if len(parts) == 2:
                fname, text = parts
                mapping[fname.strip()] = text.strip()
    if not mapping:
        raise RuntimeError(f"No ground-truth entries found in {gt_path}")
    return mapping


def make_image_list_from_gt(gt_map: Dict[str, str], image_dir: str) -> List[str]:
    """
    Ð¡Ñ‚Ñ€Ð¾Ð¸Ñ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿ÑƒÑ‚ÐµÐ¹ Ðº Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼ ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ Ð¿Ð¾Ñ€ÑÐ´ÐºÑƒ ÐºÐ»ÑŽÑ‡ÐµÐ¹ Ð² gt_map,
    Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÑ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸. ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´Ð°ÐµÑ‚ Ð¾Ð± Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ñ„Ð°Ð¹Ð»Ð°Ñ….
    """
    images: List[str] = []
    missing: List[str] = []

    for fname in gt_map.keys():  # Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ ÑÑ‚Ñ€Ð¾ÐºÐ°Ð¼ Ð² GT_FILE (Py3.7+)
        path = os.path.join(image_dir, fname)
        if os.path.isfile(path):
            images.append(path)
        else:
            missing.append(fname)

    if missing:
        print(
            f"[Warn] Ð’ GT ÑƒÐºÐ°Ð·Ð°Ð½Ð¾ {len(missing)} Ñ„Ð°Ð¹Ð»Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð½ÐµÑ‚ Ð² Ð¿Ð°Ð¿ÐºÐµ '{image_dir}':"
        )
        for m in missing[:20]:
            print("   -", m)
        if len(missing) > 20:
            print(f"   ... Ð¸ ÐµÑ‰Ñ‘ {len(missing) - 20}")

    if not images:
        raise RuntimeError("ÐŸÐ¾ GT Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ.")

    return images


def collect_images(image_dir: str) -> List[str]:
    valid_ext = {".png", ".jpg", ".jpeg", ".tif", ".tiff", ".bmp"}
    images: List[str] = []
    for fname in os.listdir(image_dir):
        path = os.path.join(image_dir, fname)
        if not os.path.isfile(path):
            continue
        if os.path.splitext(fname)[1].lower() in valid_ext:
            images.append(path)
    images.sort()
    if not images:
        raise RuntimeError(f"No images with valid extensions found in {image_dir}")
    return images


def evaluate_avg_cer(
    recognizer: TRBA,
    images: List[str],
    gt_map: Dict[str, str],
    batch_size: int,
    *,
    mode: str,
    beam_size: int,
    alpha: float,
    temperature: float = 1.0,
) -> float:
    num_batches = (len(images) + batch_size - 1) // batch_size

    print(
        f"  ðŸ”„ Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ: {len(images)} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, {num_batches} Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ "
        f"(batch_size={batch_size}, mode={mode}, beam_size={beam_size})"
    )

    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€ Ð´Ð»Ñ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
    with tqdm(
        total=num_batches, desc="  ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð±Ð°Ñ‚Ñ‡ÐµÐ¹", unit="batch", leave=False
    ) as pbar:
        # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð±Ð°Ñ‚Ñ‡Ð¸ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð°
        all_predictions = []
        for i in range(0, len(images), batch_size):
            batch_images = images[i : i + batch_size]
            batch_predictions = recognizer.predict(
                images=batch_images,
                batch_size=batch_size,
                mode=mode,
                beam_size=beam_size,
                alpha=alpha,
                temperature=temperature,
            )
            all_predictions.extend(batch_predictions)
            pbar.update(1)

    predictions = all_predictions
    print(f"  âœ… ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ñ‹, Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸...")

    total_cer = 0.0
    count = 0
    for path, (pred_text, _) in zip(images, predictions):
        ref = gt_map.get(os.path.basename(path))
        if ref is None:
            continue
        total_cer += character_error_rate(ref, pred_text)
        count += 1

    if count == 0:
        raise RuntimeError("No predictions matched the ground-truth keys.")
    return total_cer / count


def evaluate_metrics(
    recognizer: TRBA,
    images: List[str],
    gt_map: Dict[str, str],
    batch_size: int,
    *,
    mode: str,
    beam_size: int,
    alpha: float,
    temperature: float,
) -> tuple[float, float]:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ (avg_cer, accuracy).
    accuracy - Ð´Ð¾Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹, Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð°Ñ Ñ‡ÐµÑ€ÐµÐ· compute_accuracy.
    """
    num_batches = (len(images) + batch_size - 1) // batch_size

    print(
        f"  ðŸ”„ Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ: {len(images)} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, {num_batches} Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ "
        f"(batch_size={batch_size}, mode={mode}, beam_size={beam_size})"
    )

    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€ Ð´Ð»Ñ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
    with tqdm(
        total=num_batches, desc="  ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð±Ð°Ñ‚Ñ‡ÐµÐ¹", unit="batch", leave=False
    ) as pbar:
        # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð±Ð°Ñ‚Ñ‡Ð¸ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð°
        all_predictions = []
        for i in range(0, len(images), batch_size):
            batch_images = images[i : i + batch_size]
            batch_predictions = recognizer.predict(
                images=batch_images,
                batch_size=batch_size,
                mode=mode,
                beam_size=beam_size,
                alpha=alpha,
                temperature=temperature,
            )
            all_predictions.extend(batch_predictions)
            pbar.update(1)

    predictions = all_predictions
    print(f"  âœ… ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ñ‹, Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸...")

    refs: List[str] = []
    hyps: List[str] = []
    total_cer = 0.0

    for path, (pred_text, _) in zip(images, predictions):
        ref = gt_map.get(os.path.basename(path))
        if ref is None:
            continue
        refs.append(ref)
        hyps.append(pred_text)
        total_cer += character_error_rate(ref, pred_text)

    if len(refs) == 0:
        raise RuntimeError("No predictions matched the ground-truth keys.")

    avg_cer = total_cer / len(refs)
    accuracy = compute_accuracy(refs, hyps)
    return avg_cer, accuracy


def maybe_launch_dashboard(study: optuna.Study) -> None:
    """
    ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ optuna-dashboard Ð¿Ñ€Ð¸ ÑÑ‚Ð°Ñ€Ñ‚Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸.
    """
    global optuna_dashboard  # âœ… ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ

    if optuna_dashboard is None:
        try:
            import optuna_dashboard  # type: ignore
        except ImportError:
            print(
                "[Dashboard] ÐÐµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ optuna-dashboard. "
                "Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸: pip install optuna-dashboard"
            )
            return

    if STORAGE is None:
        print(
            "[Dashboard] STORAGE Ð½Ðµ Ð·Ð°Ð´Ð°Ð½. "
            "Ð—Ð°Ð´Ð°Ð¹ Ð¿ÑƒÑ‚ÑŒ, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: STORAGE = 'sqlite:///trba_decode.db'"
        )
        return

    def _run_server():
        try:
            # optuna-dashboard Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ storage URL, Ð° Ð½Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚ study
            optuna_dashboard.run_server(
                STORAGE,
                host=DASHBOARD_HOST,
                port=DASHBOARD_PORT,
            )
        except Exception as e:
            print(f"[Dashboard] ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿ÑƒÑÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°: {e}")
            import traceback

            traceback.print_exc()

    # ðŸ”¥ ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÑÑ‚Ð°Ñ€Ñ‚ Dashboard Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ
    server_thread = threading.Thread(target=_run_server, daemon=True)
    server_thread.start()
    print(f"ðŸš€ Dashboard Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ Ð½Ð° http://{DASHBOARD_HOST}:{DASHBOARD_PORT}")
    print(f"ðŸ“¦ Storage: {STORAGE}")

    # Ð–Ð´Ñ‘Ð¼ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐµÑ€Ð²ÐµÑ€ ÑƒÑÐ¿ÐµÐ» Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒÑÑ
    print("â³ ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°...")
    time.sleep(3)

    # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´Ð° Ð² Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ðµ
    dashboard_url = f"http://{DASHBOARD_HOST}:{DASHBOARD_PORT}"
    try:
        webbrowser.open(dashboard_url)
        print(f"ðŸŒ Ð‘Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚ Ð½Ð° {dashboard_url}")
    except Exception as e:
        print(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚ÑŒ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€: {e}")
        print(f"   ÐžÑ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ: {dashboard_url}")


def main():
    print("ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ground truth...")
    gt_map = load_ground_truth(GT_FILE)
    print(f"   Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(gt_map)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ ground truth")

    print("ðŸ–¼ï¸  Ð¡Ð±Ð¾Ñ€ÐºÐ° ÑÐ¿Ð¸ÑÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹...")
    images = make_image_list_from_gt(gt_map, IMAGE_DIR)
    print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(images)} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹\n")

    print("ðŸ¤– Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ TRBA...")
    recognizer = TRBA(
        model_path=MODEL_PATH,
        config_path=CONFIG_PATH,
        charset_path=CHARSET_PATH,
    )
    print("   ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°\n")

    baseline_params = {
        "mode": "greedy",
        "beam_size": 1,
        "alpha": 0.0,
    }
    print(f"\n[Baseline evaluation] Ð—Ð°Ð¿ÑƒÑÐº Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸: {baseline_params}")
    baseline_cer = evaluate_avg_cer(
        recognizer,
        images,
        gt_map,
        BATCH_SIZE,
        **baseline_params,
    )
    print(f"[Baseline] mode=greedy Avg CER={baseline_cer:.4f}\n")

    sampler = optuna.samplers.TPESampler(seed=SEED)
    study_kwargs = {
        "study_name": STUDY_NAME,
        "direction": "maximize",
        "sampler": sampler,
    }
    if STORAGE:
        study_kwargs["storage"] = STORAGE
        study_kwargs["load_if_exists"] = True

    study = optuna.create_study(**study_kwargs)
    study.enqueue_trial(baseline_params)
    maybe_launch_dashboard(study)

    def objective(trial: optuna.Trial) -> float:
        mode = trial.suggest_categorical("mode", ["greedy", "beam"])

        if mode == "greedy":
            # Ð”Ð»Ñ greedy Ñ€ÐµÐ¶Ð¸Ð¼Ð° beam-ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹
            beam_size = 1
            alpha = 0.0
            temperature = 1.0
            # normalize_by_length = True
            # diverse_groups = 1
            # diversity_strength = 0.0
            # noise_level = 0.3
            # topk_sampling_steps = 3
            # topk = 5
            # coverage_penalty_weight = 0.1
            # expand_beam_steps = 3
        else:
            # Ð”Ð»Ñ beam Ñ€ÐµÐ¶Ð¸Ð¼Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
            beam_size = trial.suggest_int("beam_size", 2, 12)
            alpha = trial.suggest_float("alpha", 0.0, 1.0)
            temperature = trial.suggest_float("temperature", 0.7, 2.0)
            # normalize_by_length = True
            # diverse_groups = trial.suggest_int("diverse_groups", 1, 4)
            # diversity_strength = trial.suggest_float("diversity_strength", 0.0, 2.0)
            # noise_level = trial.suggest_float("noise_level", 0.0, 1.0)
            # topk_sampling_steps = trial.suggest_int("topk_sampling_steps", 0, 10)
            # topk = trial.suggest_int("topk", 1, 20)
            # coverage_penalty_weight = trial.suggest_float(
            #     "coverage_penalty_weight", 0.0, 1.0
            # )
            # expand_beam_steps = trial.suggest_int("expand_beam_steps", 0, 10)

        avg_cer, accuracy = evaluate_metrics(
            recognizer,
            images,
            gt_map,
            BATCH_SIZE,
            mode=mode,
            beam_size=beam_size,
            alpha=alpha,
            temperature=temperature,
        )

        if mode == "greedy":
            print(
                f"[Trial {trial.number}] mode=greedy -> CER={avg_cer:.4f} Acc={accuracy:.4f}"
            )
        else:
            print(
                f"[Trial {trial.number}] mode=beam beam={beam_size} alpha={alpha:.2f} "
                f"-> CER={avg_cer:.4f} Acc={accuracy:.4f}"
            )
        return accuracy

    study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)

    best_params = study.best_params
    best_value = study.best_value
    print("\n=== Optuna Results ===")
    print(f"Best accuracy: {best_value:.4f}")
    for key, value in best_params.items():
        print(f"{key}: {value}")

    print("\nRe-evaluating best parameters...")
    best_cer, best_accuracy = evaluate_metrics(
        recognizer,
        images,
        gt_map,
        BATCH_SIZE,
        mode=best_params.get("mode", "beam"),
        beam_size=best_params.get("beam_size", 5),
        alpha=best_params.get("alpha", 0.0),
        temperature=best_params.get("temperature", 1.0),
    )
    print(f"Confirmed CER: {best_cer:.4f}    Confirmed accuracy: {best_accuracy:.4f}")

    if ENABLE_DASHBOARD and optuna_dashboard is not None and STORAGE is not None:
        print(
            f"[Dashboard] Keep monitoring at http://{DASHBOARD_HOST}:{DASHBOARD_PORT} "
            "or stop the script to close it."
        )


if __name__ == "__main__":
    main()
